{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this script:\n",
    "Find mean vectors in the latent space, for each modality of our variables (age, gender)\n",
    "\n",
    "in order to make some modifications to data, and challenge our classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Path and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__\t\t\tmodels.py\r\n",
      "data\t\t\t\tresults\r\n",
      "latent_space_exploration.ipynb\tspectrogram_classifier.ipynb\r\n",
      "modality_shift.ipynb\t\tsrc\r\n",
      "models\r\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "#path = \"drive/MyDrive/projet_digitale/Spectrogram_Reconstruction_Model\"\n",
    "\n",
    "#os.chdir(path)\n",
    "\n",
    "!ls\n",
    "\n",
    "sys.path.append(os.getcwd() + os.sep + 'src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.io.wavfile import write as write_waveform\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import glob2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from spectrogram_stream import SpectrogramStream\n",
    "from autoencoders import ConvolutionalAutoencoder\n",
    "from encoders import ConvolutionalEncoder\n",
    "from bottlenecks import ConvolutionalBottleneck\n",
    "from reconstructors import ConvolutionalDecoder\n",
    "from visualization import spectrogram_to_waveform, compute_reconstruction_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save/load encoded data\n",
    "save = False\n",
    "load = True\n",
    "if save :\n",
    "  pickle.dump(projection, open(\"data/\" + \"projection.pickle\", \"wb\"))\n",
    "  pickle.dump(sound_id, open(\"data/\" + \"sound_id.pickle\", \"wb\"))\n",
    "\n",
    "if load:\n",
    "  projection = pickle.load(open(\"data/\" + \"projection.pickle\", \"rb\"))\n",
    "  sound_id = pickle.load(open(\"data/\" + \"sound_id.pickle\", \"rb\"))\n",
    "    \n",
    "projection = np.array(projection)\n",
    "projection.shape\n",
    "\n",
    "# format in dataframe with ids\n",
    "representation = pd.DataFrame(projection, index=sound_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load labels file\n",
    "label_df = pd.read_csv(\"data/labels.tsv\", sep='\\t').drop(columns=\"Unnamed: 0\").set_index(\"sound_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Building model...\n",
      "Searching last checkpoint...\n",
      "Looking for checkpoints at models/dataset2filtered_b64_baseline_larger_l1_checkpoint_*.pth\n",
      "Loading checkpoint models/dataset2filtered_b64_baseline_larger_l1_checkpoint_126633.pth...\n",
      "Checkpoint models/dataset2filtered_b64_baseline_larger_l1_checkpoint_126633.pth loaded\n",
      "Loading checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "def find_last_checkpoint(models_path, experiment_name):\n",
    "    checkpoint_path_pattern = os.path.join(models_path, experiment_name + '_checkpoint_*.pth')\n",
    "    print(f'Looking for checkpoints at {checkpoint_path_pattern}')\n",
    "    checkpoints = glob2.glob(checkpoint_path_pattern)\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(os.path.splitext(x)[0].split('_')[-1]))\n",
    "    print(f'Loading checkpoint {checkpoints[-1]}...')\n",
    "    last_checkpoint = torch.load(checkpoints[-1], map_location=device)\n",
    "    print(f'Checkpoint {checkpoints[-1]} loaded')\n",
    "    return last_checkpoint\n",
    "\n",
    "data_path = 'data'\n",
    "models_path = 'models'\n",
    "experiment_name = 'dataset2filtered_b64_baseline_larger_l1'\n",
    "results_path = os.path.join('results', experiment_name)\n",
    "frame_step = 46\n",
    "n_iter = 300\n",
    "sampling_rate = 16000\n",
    "n_images = 10\n",
    "\n",
    "pathlib.Path(results_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Select available device\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda:0\")#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "\n",
    "# Build and load model\n",
    "print('Building model...')\n",
    "model = ConvolutionalAutoencoder(\n",
    "    encoder=ConvolutionalEncoder(),\n",
    "    bottleneck=ConvolutionalBottleneck(),\n",
    "    reconstructor=ConvolutionalDecoder()\n",
    ").to(device)\n",
    "\n",
    "print('Searching last checkpoint...')\n",
    "checkpoint = find_last_checkpoint(models_path, experiment_name)\n",
    "\n",
    "print('Loading checkpoint...')\n",
    "new_state_dict = OrderedDict()\n",
    "for mod in ['model', 'optimizer']:\n",
    "    new_state_dict[mod] = {}\n",
    "    for k, v in checkpoint[mod if mod in checkpoint else mod + '_exception'].items():\n",
    "        new_state_dict[mod][k.replace('module.', '')] = v\n",
    "checkpoint = new_state_dict\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Shift representation with mean modality vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_vector(var = \"gender\",mod = 1):\n",
    "    \"\"\"\n",
    "    function to compute mean vector for chosen variable and modality\n",
    "    \"\"\"\n",
    "    # select ids for which we have variable = modality\n",
    "    ids = label_df.loc[label_df[var] == mod].index\n",
    "    \n",
    "    # select latent representation for selected ids\n",
    "    latent_df = representation.loc[representation.index.isin(ids)]\n",
    "    \n",
    "    return np.array(latent_df.apply(np.mean,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and save mean vector for age and gender\n",
    "for var in [\"gender\", \"age\"]:\n",
    "    for mod in [0,1,2]:\n",
    "        \n",
    "        if var == \"gender\" and mod == 2:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            mean = compute_mean_vector(var, mod)\n",
    "            pickle.dump(mean, open(\"data/\" + \"mean_\" + var + str(mod) + \".pickle\", \"wb\"))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift our data & save it in \"projection_shift_age01.pickle\"\n",
    "for var in [\"gender\", \"age\"]:\n",
    "    for start_mod in [0,1,2]:\n",
    "        for end_mod in [0,1,2]:\n",
    "\n",
    "            if (var == \"gender\" and (start_mod == 2 or end_mod == 2)) or (start_mod == end_mod):\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                # read file with mean vectors for the 2 modalities\n",
    "                file0,file1 = \"data/\" + \"mean_\" + var + str(start_mod) + \".pickle\", \"data/\" + \"mean_\" + var + str(end_mod) + \".pickle\"\n",
    "                start_vector, end_vector = pickle.load( open(file0, \"rb\")), pickle.load( open(file1, \"rb\"))\n",
    "\n",
    "                # Shift our data in latent space \n",
    "                #(for convenience, we shift all our data using vectorial difference between 2 modalities)\n",
    "                file = \"data/\" + \"projection_shift_\" + var + str(start_mod) + str(end_mod) + \".pickle\"\n",
    "                modified_representation = representation + (end_vector - start_vector)\n",
    "                pickle.dump(modified_representation, open(file, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reconstruct spectrograms from modified representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(var=\"age\",start_mod=0, end_mod = 1,N=50):\n",
    "    \"\"\"\n",
    "    load shifted data, reconstruct spectrograms and save results in pickle file\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    file = \"data/\" + \"projection_shift_\" + var + str(start_mod) + str(end_mod) + \".pickle\"\n",
    "    representation = pickle.load(open(file,\"rb\"))\n",
    "    \n",
    "    # cast the latent representation into correct shape for reconstruction, and cast in torch tensor\n",
    "    representation = torch.Tensor(representation.to_numpy().reshape(-1,1,24,11))\n",
    "    representation = representation.to(device)\n",
    "    #print(representation.shape)\n",
    "    \n",
    "    # reconstruct\n",
    "    reconstruction_list= []\n",
    "    for n in range(0, representation.shape[0]//N +1 ):\n",
    "        with torch.no_grad():\n",
    "            reconstruction = model.reconstructor.forward(representation[n*N:(n+1)*N])\n",
    "\n",
    "        reconstruction_list.append(reconstruction)#reconstruction.to(\"cpu\"))\n",
    "        print(n)\n",
    "        #del reconstruction\n",
    "        if n%N == 0:\n",
    "            print(n)\n",
    "        #    torch.cuda.empty_cache()\n",
    "    \n",
    "    # save\n",
    "    file_out = \"data/\" + \"reconstruction_shifted_\" + var + str(start_mod) + str(end_mod) + \".pickle\"\n",
    "    pickle.dump(torch.cat(tuple(reconstruction_list)).numpy(), open(file_out,\"wb\"))\n",
    "    \n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-67c0ee5d6d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_mod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-5b7427b32c7d>\u001b[0m in \u001b[0;36mreconstruct\u001b[0;34m(var, start_mod, end_mod, N)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstructor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mreconstruction_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#reconstruction.to(\"cpu\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/Spectrogram_Reconstruction_Model/src/reconstructors.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    840\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    841\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for var in [\"gender\", \"age\"]:\n",
    "    for start_mod in [0,1,2]:\n",
    "        for end_mod in [0,1,2]:\n",
    "\n",
    "            if (var == \"gender\" and (start_mod == 2 or end_mod == 2)) or (start_mod == end_mod):\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                reconstruct(var=var,start_mod=start_mod, end_mod=end_mod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
